{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python [conda env:pytorch]","language":"python","name":"conda-env-pytorch-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"Train_cifar.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"recent-military","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620899526339,"user_tz":-60,"elapsed":6095999,"user":{"displayName":"Xiliang He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6bak5pEgJC5_hHvho9fDsSh27vhZUOZYg8FCDOg=s64","userId":"00653517590094465678"}},"outputId":"96c094a8-5431-4247-cdb4-53e1068b305b"},"source":["from __future__ import print_function\n","import sys\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","import random\n","import os\n","import argparse\n","import numpy as np\n","from PreResNet import *\n","from sklearn.mixture import GaussianMixture\n","\n","try:\n","    from torchnet.meter import AUCMeter\n","except:\n","    !pip install torchnet\n","    from torchnet.meter import AUCMeter\n","\n","import dataloader_cifar as dataloader\n","\n","parser = argparse.ArgumentParser(description='PyTorch CIFAR Training')\n","parser.add_argument('--batch_size', default=64, type=int, help='train batchsize') \n","parser.add_argument('--lr', '--learning_rate', default=0.02, type=float, help='initial learning rate')\n","parser.add_argument('--noise_mode',  default='sym')\n","parser.add_argument('--alpha', default=4, type=float, help='parameter for Beta')\n","parser.add_argument('--lambda_u', default=25, type=float, help='weight for unsupervised loss')\n","parser.add_argument('--p_threshold', default=0.5, type=float, help='clean probability threshold')\n","parser.add_argument('--T', default=0.5, type=float, help='sharpening temperature')\n","parser.add_argument('--num_epochs', default=100, type=int)\n","parser.add_argument('--r', default=0.5, type=float, help='noise ratio')\n","parser.add_argument('--id', default='')\n","parser.add_argument('--seed', default=123)\n","parser.add_argument('--gpuid', default=0, type=int)\n","parser.add_argument('--num_class', default=10, type=int)\n","parser.add_argument('--data_path', default='./cifar-10', type=str, help='path to dataset')\n","parser.add_argument('--dataset', default='cifar10', type=str)\n","# args = parser.parse_args()\n","args, unknown = parser.parse_known_args()\n","\n","torch.cuda.set_device(args.gpuid)\n","random.seed(args.seed)\n","torch.manual_seed(args.seed)\n","torch.cuda.manual_seed_all(args.seed)\n","\n","\n","# Training\n","def train(epoch,net,net2,optimizer,labeled_trainloader,unlabeled_trainloader):\n","    net.train()\n","    net2.eval() #fix one network and train the other\n","    \n","    unlabeled_train_iter = iter(unlabeled_trainloader)    \n","    num_iter = (len(labeled_trainloader.dataset)//args.batch_size)+1\n","    for batch_idx, (inputs_x, inputs_x2, labels_x, w_x) in enumerate(labeled_trainloader):      \n","        try:\n","            inputs_u, inputs_u2 = unlabeled_train_iter.next()\n","        except:\n","            unlabeled_train_iter = iter(unlabeled_trainloader)\n","            inputs_u, inputs_u2 = unlabeled_train_iter.next()                 \n","        batch_size = inputs_x.size(0)\n","        \n","        # Transform label to one-hot\n","        labels_x = torch.zeros(batch_size, args.num_class).scatter_(1, labels_x.view(-1,1), 1)        \n","        w_x = w_x.view(-1,1).type(torch.FloatTensor) \n","\n","        inputs_x, inputs_x2, labels_x, w_x = inputs_x.cuda(), inputs_x2.cuda(), labels_x.cuda(), w_x.cuda()\n","        inputs_u, inputs_u2 = inputs_u.cuda(), inputs_u2.cuda()\n","\n","        with torch.no_grad():\n","            # label co-guessing of unlabeled samples\n","            outputs_u11 = net(inputs_u)\n","            outputs_u12 = net(inputs_u2)\n","            outputs_u21 = net2(inputs_u)\n","            outputs_u22 = net2(inputs_u2)            \n","            \n","            pu = (torch.softmax(outputs_u11, dim=1) + torch.softmax(outputs_u12, dim=1) + torch.softmax(outputs_u21, dim=1) + torch.softmax(outputs_u22, dim=1)) / 4       \n","            ptu = pu**(1/args.T) # temparature sharpening\n","            \n","            targets_u = ptu / ptu.sum(dim=1, keepdim=True) # normalize\n","            targets_u = targets_u.detach()       \n","            \n","            # label refinement of labeled samples\n","            outputs_x = net(inputs_x)\n","            outputs_x2 = net(inputs_x2)            \n","            \n","            px = (torch.softmax(outputs_x, dim=1) + torch.softmax(outputs_x2, dim=1)) / 2\n","            px = w_x*labels_x + (1-w_x)*px              \n","            ptx = px**(1/args.T) # temparature sharpening \n","                       \n","            targets_x = ptx / ptx.sum(dim=1, keepdim=True) # normalize           \n","            targets_x = targets_x.detach()       \n","        \n","        # mixmatch\n","        l = np.random.beta(args.alpha, args.alpha)        \n","        l = max(l, 1-l)\n","                \n","        all_inputs = torch.cat([inputs_x, inputs_x2, inputs_u, inputs_u2], dim=0)\n","        all_targets = torch.cat([targets_x, targets_x, targets_u, targets_u], dim=0)\n","\n","        idx = torch.randperm(all_inputs.size(0))\n","\n","        input_a, input_b = all_inputs, all_inputs[idx]\n","        target_a, target_b = all_targets, all_targets[idx]\n","        \n","        mixed_input = l * input_a + (1 - l) * input_b        \n","        mixed_target = l * target_a + (1 - l) * target_b\n","                \n","        logits = net(mixed_input)\n","        logits_x = logits[:batch_size*2]\n","        logits_u = logits[batch_size*2:]        \n","           \n","        Lx, Lu, lamb = criterion(logits_x, mixed_target[:batch_size*2], logits_u, mixed_target[batch_size*2:], epoch+batch_idx/num_iter, warm_up)\n","        \n","        # regularization\n","        prior = torch.ones(args.num_class)/args.num_class\n","        prior = prior.cuda()        \n","        pred_mean = torch.softmax(logits, dim=1).mean(0)\n","        penalty = torch.sum(prior*torch.log(prior/pred_mean))\n","\n","        loss = Lx + lamb * Lu  + penalty\n","        # compute gradient and do SGD step\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        sys.stdout.write('\\r')\n","        sys.stdout.write('%s:%.1f-%s | Epoch [%3d/%3d] Iter[%3d/%3d]\\t Labeled loss: %.4f  Unlabeled loss: %.4f'\n","                %(args.dataset, args.r, args.noise_mode, epoch, args.num_epochs, batch_idx+1, num_iter, Lx.item(), Lu.item()))\n","        sys.stdout.flush()\n","\n","def warmup(epoch,net,optimizer,dataloader):\n","    net.train()\n","    num_iter = (len(dataloader.dataset)//dataloader.batch_size)+1\n","    for batch_idx, (inputs, labels, path) in enumerate(dataloader):      \n","        inputs, labels = inputs.cuda(), labels.cuda() \n","        optimizer.zero_grad()\n","        outputs = net(inputs)               \n","        loss = CEloss(outputs, labels)      \n","        if args.noise_mode=='asym':  # penalize confident prediction for asymmetric noise\n","            penalty = conf_penalty(outputs)\n","            L = loss + penalty      \n","        elif args.noise_mode=='sym':   \n","            L = loss\n","        L.backward()  \n","        optimizer.step() \n","\n","        sys.stdout.write('\\r')\n","        sys.stdout.write('%s:%.1f-%s | Epoch [%3d/%3d] Iter[%3d/%3d]\\t CE-loss: %.4f'\n","                %(args.dataset, args.r, args.noise_mode, epoch, args.num_epochs, batch_idx+1, num_iter, loss.item()))\n","        sys.stdout.flush()\n","\n","def test(epoch,net1,net2):\n","    net1.eval()\n","    net2.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(test_loader):\n","            inputs, targets = inputs.cuda(), targets.cuda()\n","            outputs1 = net1(inputs)\n","            outputs2 = net2(inputs)           \n","            outputs = outputs1+outputs2\n","            _, predicted = torch.max(outputs, 1)            \n","                       \n","            total += targets.size(0)\n","            correct += predicted.eq(targets).cpu().sum().item()                 \n","    acc = 100.*correct/total\n","    print(\"\\n| Test Epoch #%d\\t Accuracy: %.2f%%\\n\" %(epoch,acc))  \n","    test_log.write('Epoch:%d   Accuracy:%.2f\\n'%(epoch,acc))\n","    test_log.flush()  \n","\n","def eval_train(model,all_loss):    \n","    model.eval()\n","    losses = torch.zeros(50000)    \n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets, index) in enumerate(eval_loader):\n","            inputs, targets = inputs.cuda(), targets.cuda() \n","            outputs = model(inputs) \n","            loss = CE(outputs, targets)  \n","            for b in range(inputs.size(0)):\n","                losses[index[b]]=loss[b]         \n","    losses = (losses-losses.min())/(losses.max()-losses.min())    \n","    all_loss.append(losses)\n","\n","    if args.r==0.9: # average loss over last 5 epochs to improve convergence stability\n","        history = torch.stack(all_loss)\n","        input_loss = history[-5:].mean(0)\n","        input_loss = input_loss.reshape(-1,1)\n","    else:\n","        input_loss = losses.reshape(-1,1)\n","    \n","    # fit a two-component GMM to the loss\n","    gmm = GaussianMixture(n_components=2,max_iter=10,tol=1e-2,reg_covar=5e-4)\n","    gmm.fit(input_loss)\n","    prob = gmm.predict_proba(input_loss) \n","    prob = prob[:,gmm.means_.argmin()]         \n","    return prob,all_loss\n","\n","def linear_rampup(current, warm_up, rampup_length=16):\n","    current = np.clip((current-warm_up) / rampup_length, 0.0, 1.0)\n","    return args.lambda_u*float(current)\n","\n","class SemiLoss(object):\n","    def __call__(self, outputs_x, targets_x, outputs_u, targets_u, epoch, warm_up):\n","        probs_u = torch.softmax(outputs_u, dim=1)\n","\n","        Lx = -torch.mean(torch.sum(F.log_softmax(outputs_x, dim=1) * targets_x, dim=1))\n","        Lu = torch.mean((probs_u - targets_u)**2)\n","\n","        return Lx, Lu, linear_rampup(epoch,warm_up)\n","\n","class NegEntropy(object):\n","    def __call__(self,outputs):\n","        probs = torch.softmax(outputs, dim=1)\n","        return torch.mean(torch.sum(probs.log()*probs, dim=1))\n","\n","def create_model():\n","    model = ResNet18(num_classes=args.num_class)\n","    model = model.cuda()\n","    return model\n","\n","stats_log=open('./checkpoint/%s_%.1f_%s'%(args.dataset,args.r,args.noise_mode)+'_stats.txt','w') \n","test_log=open('./checkpoint/%s_%.1f_%s'%(args.dataset,args.r,args.noise_mode)+'_acc.txt','w')     \n","\n","if args.dataset=='cifar10':\n","    warm_up = 10\n","elif args.dataset=='cifar100':\n","    warm_up = 30\n","\n","loader = dataloader.cifar_dataloader(args.dataset,r=args.r,noise_mode=args.noise_mode,batch_size=args.batch_size,num_workers=2,\\\n","    root_dir=args.data_path,log=stats_log,noise_file='%s/%.1f_%s.json'%(args.data_path,args.r,args.noise_mode))\n","\n","print('| Building net')\n","net1 = create_model()\n","net2 = create_model()\n","cudnn.benchmark = True\n","\n","criterion = SemiLoss()\n","optimizer1 = optim.SGD(net1.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n","optimizer2 = optim.SGD(net2.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n","\n","CE = nn.CrossEntropyLoss(reduction='none')\n","CEloss = nn.CrossEntropyLoss()\n","if args.noise_mode=='asym':\n","    conf_penalty = NegEntropy()\n","\n","all_loss = [[],[]] # save the history of losses from two networks\n","\n","for epoch in range(args.num_epochs+1):   \n","    lr=args.lr\n","    if epoch >= 150:\n","        lr /= 10      \n","    for param_group in optimizer1.param_groups:\n","        param_group['lr'] = lr       \n","    for param_group in optimizer2.param_groups:\n","        param_group['lr'] = lr          \n","    test_loader = loader.run('test')\n","    eval_loader = loader.run('eval_train')   \n","    \n","    if epoch<warm_up:       \n","        warmup_trainloader = loader.run('warmup')\n","        print('Warmup Net1')\n","        warmup(epoch,net1,optimizer1,warmup_trainloader)    \n","        print('\\nWarmup Net2')\n","        warmup(epoch,net2,optimizer2,warmup_trainloader) \n","   \n","    else:         \n","        prob1,all_loss[0]=eval_train(net1,all_loss[0])   \n","        prob2,all_loss[1]=eval_train(net2,all_loss[1])          \n","               \n","        pred1 = (prob1 > args.p_threshold)      \n","        pred2 = (prob2 > args.p_threshold)      \n","        \n","        print('Train Net1')\n","        labeled_trainloader, unlabeled_trainloader = loader.run('train',pred2,prob2) # co-divide\n","        train(epoch,net1,net2,optimizer1,labeled_trainloader, unlabeled_trainloader) # train net1  \n","        \n","        print('\\nTrain Net2')\n","        labeled_trainloader, unlabeled_trainloader = loader.run('train',pred1,prob1) # co-divide\n","        train(epoch,net2,net1,optimizer2,labeled_trainloader, unlabeled_trainloader) # train net2         \n","\n","    test(epoch,net1,net2)"],"id":"recent-military","execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting torchnet\n","  Downloading https://files.pythonhosted.org/packages/b7/b2/d7f70a85d3f6b0365517782632f150e3bbc2fb8e998cd69e27deba599aae/torchnet-0.0.4.tar.gz\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchnet) (1.8.1+cu101)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchnet) (1.15.0)\n","Collecting visdom\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/75/e078f5a2e1df7e0d3044749089fc2823e62d029cc027ed8ae5d71fafcbdc/visdom-0.1.8.9.tar.gz (676kB)\n","\u001b[K     |████████████████████████████████| 686kB 41.7MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchnet) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->torchnet) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from visdom->torchnet) (1.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom->torchnet) (2.23.0)\n","Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom->torchnet) (5.1.1)\n","Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom->torchnet) (22.0.3)\n","Collecting jsonpatch\n","  Downloading https://files.pythonhosted.org/packages/a3/55/f7c93bae36d869292aedfbcbae8b091386194874f16390d680136edd2b28/jsonpatch-1.32-py2.py3-none-any.whl\n","Collecting torchfile\n","  Downloading https://files.pythonhosted.org/packages/91/af/5b305f86f2d218091af657ddb53f984ecbd9518ca9fe8ef4103a007252c9/torchfile-0.1.0.tar.gz\n","Collecting websocket-client\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/0c/d52a2a63512a613817846d430d16a8fbe5ea56dd889e89c68facf6b91cb6/websocket_client-0.59.0-py2.py3-none-any.whl (67kB)\n","\u001b[K     |████████████████████████████████| 71kB 10.1MB/s \n","\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from visdom->torchnet) (7.1.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->visdom->torchnet) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->visdom->torchnet) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom->torchnet) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom->torchnet) (3.0.4)\n","Collecting jsonpointer>=1.9\n","  Downloading https://files.pythonhosted.org/packages/23/52/05f67532aa922e494c351344e0d9624a01f74f5dd8402fe0d1b563a6e6fc/jsonpointer-2.1-py2.py3-none-any.whl\n","Building wheels for collected packages: torchnet, visdom, torchfile\n","  Building wheel for torchnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchnet: filename=torchnet-0.0.4-cp37-none-any.whl size=29743 sha256=9ab5e584d07e9c926e72f1757d13008a83474ed367c66f2e957f59440b7ac004\n","  Stored in directory: /root/.cache/pip/wheels/e1/03/fb/1c212c2f20905cdf97fe39022946cf16b8e66ed754a6663400\n","  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for visdom: filename=visdom-0.1.8.9-cp37-none-any.whl size=655251 sha256=c4c8e61205333331de858e49bceaa40586676483937ef313d03bc23b60780ad4\n","  Stored in directory: /root/.cache/pip/wheels/70/19/a7/6d589ed967f4dfefd33bc166d081257bd4ed0cb618dccfd62a\n","  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torchfile: filename=torchfile-0.1.0-cp37-none-any.whl size=5713 sha256=ec3071645245c3570b75174bfb61fd7ef78720e644b7c665148ae8b1ef3dcdfb\n","  Stored in directory: /root/.cache/pip/wheels/b1/c3/d6/9a1cc8f3a99a0fc1124cae20153f36af59a6e683daca0a0814\n","Successfully built torchnet visdom torchfile\n","Installing collected packages: jsonpointer, jsonpatch, torchfile, websocket-client, visdom, torchnet\n","Successfully installed jsonpatch-1.32 jsonpointer-2.1 torchfile-0.1.0 torchnet-0.0.4 visdom-0.1.8.9 websocket-client-0.59.0\n","| Building net\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [  0/100] Iter[391/391]\t CE-loss: 2.2081\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [  0/100] Iter[391/391]\t CE-loss: 2.1493\n","| Test Epoch #0\t Accuracy: 50.13%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [  1/100] Iter[391/391]\t CE-loss: 2.0553\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [  1/100] Iter[391/391]\t CE-loss: 2.0039\n","| Test Epoch #1\t Accuracy: 57.61%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [  2/100] Iter[391/391]\t CE-loss: 1.8846\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [  2/100] Iter[391/391]\t CE-loss: 1.8297\n","| Test Epoch #2\t Accuracy: 61.82%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [  3/100] Iter[391/391]\t CE-loss: 1.9028\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [  3/100] Iter[391/391]\t CE-loss: 2.1387\n","| Test Epoch #3\t Accuracy: 64.51%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [  4/100] Iter[391/391]\t CE-loss: 2.0429\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [  4/100] Iter[391/391]\t CE-loss: 2.0336\n","| Test Epoch #4\t Accuracy: 68.16%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [  5/100] Iter[391/391]\t CE-loss: 1.7621\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [  5/100] Iter[391/391]\t CE-loss: 2.0261\n","| Test Epoch #5\t Accuracy: 70.80%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [  6/100] Iter[391/391]\t CE-loss: 1.8022\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [  6/100] Iter[391/391]\t CE-loss: 2.0215\n","| Test Epoch #6\t Accuracy: 73.93%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [  7/100] Iter[391/391]\t CE-loss: 1.9812\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [  7/100] Iter[391/391]\t CE-loss: 1.7966\n","| Test Epoch #7\t Accuracy: 76.07%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [  8/100] Iter[391/391]\t CE-loss: 1.7435\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [  8/100] Iter[391/391]\t CE-loss: 1.8513\n","| Test Epoch #8\t Accuracy: 75.88%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [  9/100] Iter[391/391]\t CE-loss: 1.8170\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [  9/100] Iter[391/391]\t CE-loss: 2.0478\n","| Test Epoch #9\t Accuracy: 78.95%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 10/100] Iter[391/391]\t CE-loss: 2.0802\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 10/100] Iter[391/391]\t CE-loss: 1.9918\n","| Test Epoch #10\t Accuracy: 78.71%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 11/100] Iter[391/391]\t CE-loss: 1.7437\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 11/100] Iter[391/391]\t CE-loss: 1.7600\n","| Test Epoch #11\t Accuracy: 80.12%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 12/100] Iter[391/391]\t CE-loss: 1.8479\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 12/100] Iter[391/391]\t CE-loss: 1.7720\n","| Test Epoch #12\t Accuracy: 80.10%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 13/100] Iter[391/391]\t CE-loss: 1.7292\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 13/100] Iter[391/391]\t CE-loss: 1.7302\n","| Test Epoch #13\t Accuracy: 79.35%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 14/100] Iter[391/391]\t CE-loss: 1.9013\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 14/100] Iter[391/391]\t CE-loss: 2.0201\n","| Test Epoch #14\t Accuracy: 81.50%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 15/100] Iter[391/391]\t CE-loss: 1.7817\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 15/100] Iter[391/391]\t CE-loss: 1.6763\n","| Test Epoch #15\t Accuracy: 82.70%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 16/100] Iter[391/391]\t CE-loss: 1.7919\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 16/100] Iter[391/391]\t CE-loss: 1.9007\n","| Test Epoch #16\t Accuracy: 81.11%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 17/100] Iter[391/391]\t CE-loss: 1.7466\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 17/100] Iter[391/391]\t CE-loss: 1.7175\n","| Test Epoch #17\t Accuracy: 80.90%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 18/100] Iter[391/391]\t CE-loss: 1.8334\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 18/100] Iter[391/391]\t CE-loss: 1.7929\n","| Test Epoch #18\t Accuracy: 82.31%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 19/100] Iter[391/391]\t CE-loss: 1.7822\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 19/100] Iter[391/391]\t CE-loss: 1.8825\n","| Test Epoch #19\t Accuracy: 82.66%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 20/100] Iter[391/391]\t CE-loss: 2.1255\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 20/100] Iter[391/391]\t CE-loss: 2.0146\n","| Test Epoch #20\t Accuracy: 83.28%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 21/100] Iter[391/391]\t CE-loss: 1.9769\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 21/100] Iter[391/391]\t CE-loss: 1.6368\n","| Test Epoch #21\t Accuracy: 81.62%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 22/100] Iter[391/391]\t CE-loss: 1.6868\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 22/100] Iter[391/391]\t CE-loss: 1.7500\n","| Test Epoch #22\t Accuracy: 82.81%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 23/100] Iter[391/391]\t CE-loss: 1.8046\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 23/100] Iter[391/391]\t CE-loss: 1.7649\n","| Test Epoch #23\t Accuracy: 84.85%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 24/100] Iter[391/391]\t CE-loss: 1.8091\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 24/100] Iter[391/391]\t CE-loss: 1.6357\n","| Test Epoch #24\t Accuracy: 83.92%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 25/100] Iter[391/391]\t CE-loss: 1.5948\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 25/100] Iter[391/391]\t CE-loss: 1.5194\n","| Test Epoch #25\t Accuracy: 83.80%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 26/100] Iter[391/391]\t CE-loss: 1.8501\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 26/100] Iter[391/391]\t CE-loss: 1.6914\n","| Test Epoch #26\t Accuracy: 83.22%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 27/100] Iter[391/391]\t CE-loss: 1.6800\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 27/100] Iter[391/391]\t CE-loss: 2.0173\n","| Test Epoch #27\t Accuracy: 82.22%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 28/100] Iter[391/391]\t CE-loss: 1.6526\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 28/100] Iter[391/391]\t CE-loss: 1.6947\n","| Test Epoch #28\t Accuracy: 82.52%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 29/100] Iter[391/391]\t CE-loss: 1.7544\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 29/100] Iter[391/391]\t CE-loss: 1.6849\n","| Test Epoch #29\t Accuracy: 82.43%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 30/100] Iter[391/391]\t CE-loss: 1.8444\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 30/100] Iter[391/391]\t CE-loss: 1.6120\n","| Test Epoch #30\t Accuracy: 82.86%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 31/100] Iter[391/391]\t CE-loss: 1.6593\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 31/100] Iter[391/391]\t CE-loss: 1.6064\n","| Test Epoch #31\t Accuracy: 83.34%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 32/100] Iter[391/391]\t CE-loss: 1.8656\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 32/100] Iter[391/391]\t CE-loss: 1.7748\n","| Test Epoch #32\t Accuracy: 83.08%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 33/100] Iter[391/391]\t CE-loss: 1.5447\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 33/100] Iter[391/391]\t CE-loss: 1.7033\n","| Test Epoch #33\t Accuracy: 82.66%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 34/100] Iter[391/391]\t CE-loss: 1.6192\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 34/100] Iter[391/391]\t CE-loss: 1.6384\n","| Test Epoch #34\t Accuracy: 82.82%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 35/100] Iter[391/391]\t CE-loss: 1.5543\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 35/100] Iter[391/391]\t CE-loss: 1.7274\n","| Test Epoch #35\t Accuracy: 83.02%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 36/100] Iter[391/391]\t CE-loss: 1.7904\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 36/100] Iter[391/391]\t CE-loss: 1.8164\n","| Test Epoch #36\t Accuracy: 81.78%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 37/100] Iter[391/391]\t CE-loss: 1.8561\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 37/100] Iter[391/391]\t CE-loss: 1.6964\n","| Test Epoch #37\t Accuracy: 81.17%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 38/100] Iter[391/391]\t CE-loss: 1.6462\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 38/100] Iter[391/391]\t CE-loss: 1.6979\n","| Test Epoch #38\t Accuracy: 83.28%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 39/100] Iter[391/391]\t CE-loss: 1.7273\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 39/100] Iter[391/391]\t CE-loss: 1.5016\n","| Test Epoch #39\t Accuracy: 81.58%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 40/100] Iter[391/391]\t CE-loss: 1.6100\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 40/100] Iter[391/391]\t CE-loss: 1.7214\n","| Test Epoch #40\t Accuracy: 81.79%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 41/100] Iter[391/391]\t CE-loss: 1.6119\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 41/100] Iter[391/391]\t CE-loss: 1.4555\n","| Test Epoch #41\t Accuracy: 79.38%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 42/100] Iter[391/391]\t CE-loss: 1.6181\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 42/100] Iter[391/391]\t CE-loss: 1.4726\n","| Test Epoch #42\t Accuracy: 82.23%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 43/100] Iter[391/391]\t CE-loss: 1.8796\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 43/100] Iter[391/391]\t CE-loss: 1.5628\n","| Test Epoch #43\t Accuracy: 80.59%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 44/100] Iter[391/391]\t CE-loss: 1.6934\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 44/100] Iter[391/391]\t CE-loss: 1.6629\n","| Test Epoch #44\t Accuracy: 81.60%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 45/100] Iter[391/391]\t CE-loss: 1.9518\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 45/100] Iter[391/391]\t CE-loss: 1.6156\n","| Test Epoch #45\t Accuracy: 80.28%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 46/100] Iter[391/391]\t CE-loss: 1.4800\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 46/100] Iter[391/391]\t CE-loss: 1.6499\n","| Test Epoch #46\t Accuracy: 79.93%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 47/100] Iter[391/391]\t CE-loss: 1.3970\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 47/100] Iter[391/391]\t CE-loss: 1.5879\n","| Test Epoch #47\t Accuracy: 79.62%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 48/100] Iter[391/391]\t CE-loss: 1.6317\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 48/100] Iter[391/391]\t CE-loss: 1.4582\n","| Test Epoch #48\t Accuracy: 80.72%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 49/100] Iter[391/391]\t CE-loss: 1.4456\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 49/100] Iter[391/391]\t CE-loss: 1.6124\n","| Test Epoch #49\t Accuracy: 79.22%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 50/100] Iter[391/391]\t CE-loss: 1.5663\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 50/100] Iter[391/391]\t CE-loss: 1.6441\n","| Test Epoch #50\t Accuracy: 81.06%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 51/100] Iter[391/391]\t CE-loss: 1.3722\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 51/100] Iter[391/391]\t CE-loss: 1.5462\n","| Test Epoch #51\t Accuracy: 79.84%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 52/100] Iter[391/391]\t CE-loss: 1.4315\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 52/100] Iter[391/391]\t CE-loss: 1.5931\n","| Test Epoch #52\t Accuracy: 78.69%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 53/100] Iter[391/391]\t CE-loss: 1.2413\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 53/100] Iter[391/391]\t CE-loss: 1.2762\n","| Test Epoch #53\t Accuracy: 80.35%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 54/100] Iter[391/391]\t CE-loss: 1.6963\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 54/100] Iter[391/391]\t CE-loss: 1.6698\n","| Test Epoch #54\t Accuracy: 77.76%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 55/100] Iter[391/391]\t CE-loss: 1.6622\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 55/100] Iter[391/391]\t CE-loss: 1.4182\n","| Test Epoch #55\t Accuracy: 78.18%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 56/100] Iter[391/391]\t CE-loss: 1.4798\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 56/100] Iter[391/391]\t CE-loss: 1.3665\n","| Test Epoch #56\t Accuracy: 76.94%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 57/100] Iter[391/391]\t CE-loss: 1.5231\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 57/100] Iter[391/391]\t CE-loss: 1.7702\n","| Test Epoch #57\t Accuracy: 79.12%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 58/100] Iter[391/391]\t CE-loss: 1.6267\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 58/100] Iter[391/391]\t CE-loss: 1.5086\n","| Test Epoch #58\t Accuracy: 76.90%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 59/100] Iter[391/391]\t CE-loss: 1.2151\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 59/100] Iter[391/391]\t CE-loss: 1.7333\n","| Test Epoch #59\t Accuracy: 79.09%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 60/100] Iter[391/391]\t CE-loss: 1.3433\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 60/100] Iter[391/391]\t CE-loss: 1.4446\n","| Test Epoch #60\t Accuracy: 76.36%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 61/100] Iter[391/391]\t CE-loss: 1.5354\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 61/100] Iter[391/391]\t CE-loss: 1.3068\n","| Test Epoch #61\t Accuracy: 73.86%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 62/100] Iter[391/391]\t CE-loss: 1.3474\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 62/100] Iter[391/391]\t CE-loss: 1.4194\n","| Test Epoch #62\t Accuracy: 74.23%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 63/100] Iter[391/391]\t CE-loss: 1.6409\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 63/100] Iter[391/391]\t CE-loss: 1.5490\n","| Test Epoch #63\t Accuracy: 75.62%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 64/100] Iter[391/391]\t CE-loss: 1.4652\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 64/100] Iter[391/391]\t CE-loss: 1.3049\n","| Test Epoch #64\t Accuracy: 76.11%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 65/100] Iter[391/391]\t CE-loss: 1.5876\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 65/100] Iter[391/391]\t CE-loss: 1.2527\n","| Test Epoch #65\t Accuracy: 74.62%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 66/100] Iter[391/391]\t CE-loss: 1.0879\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 66/100] Iter[391/391]\t CE-loss: 1.3225\n","| Test Epoch #66\t Accuracy: 74.70%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 67/100] Iter[391/391]\t CE-loss: 1.3445\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 67/100] Iter[391/391]\t CE-loss: 1.0744\n","| Test Epoch #67\t Accuracy: 74.80%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 68/100] Iter[391/391]\t CE-loss: 1.4972\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 68/100] Iter[391/391]\t CE-loss: 1.3867\n","| Test Epoch #68\t Accuracy: 73.92%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 69/100] Iter[391/391]\t CE-loss: 1.5706\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 69/100] Iter[391/391]\t CE-loss: 1.2303\n","| Test Epoch #69\t Accuracy: 72.98%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 70/100] Iter[391/391]\t CE-loss: 1.2999\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 70/100] Iter[391/391]\t CE-loss: 1.4457\n","| Test Epoch #70\t Accuracy: 71.36%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 71/100] Iter[391/391]\t CE-loss: 1.1088\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 71/100] Iter[391/391]\t CE-loss: 1.2944\n","| Test Epoch #71\t Accuracy: 72.48%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 72/100] Iter[391/391]\t CE-loss: 1.3402\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 72/100] Iter[391/391]\t CE-loss: 1.2234\n","| Test Epoch #72\t Accuracy: 73.46%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 73/100] Iter[391/391]\t CE-loss: 1.4367\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 73/100] Iter[391/391]\t CE-loss: 1.4753\n","| Test Epoch #73\t Accuracy: 72.67%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 74/100] Iter[391/391]\t CE-loss: 1.4072\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 74/100] Iter[391/391]\t CE-loss: 1.2272\n","| Test Epoch #74\t Accuracy: 73.39%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 75/100] Iter[391/391]\t CE-loss: 1.2360\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 75/100] Iter[391/391]\t CE-loss: 1.0861\n","| Test Epoch #75\t Accuracy: 72.09%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 76/100] Iter[391/391]\t CE-loss: 1.1639\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 76/100] Iter[391/391]\t CE-loss: 1.3537\n","| Test Epoch #76\t Accuracy: 71.29%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 77/100] Iter[391/391]\t CE-loss: 1.1279\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 77/100] Iter[391/391]\t CE-loss: 1.0359\n","| Test Epoch #77\t Accuracy: 72.29%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 78/100] Iter[391/391]\t CE-loss: 1.0899\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 78/100] Iter[391/391]\t CE-loss: 1.3735\n","| Test Epoch #78\t Accuracy: 69.19%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 79/100] Iter[391/391]\t CE-loss: 1.0233\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 79/100] Iter[391/391]\t CE-loss: 1.1333\n","| Test Epoch #79\t Accuracy: 70.52%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 80/100] Iter[391/391]\t CE-loss: 1.1803\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 80/100] Iter[391/391]\t CE-loss: 1.0124\n","| Test Epoch #80\t Accuracy: 67.70%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 81/100] Iter[391/391]\t CE-loss: 1.1193\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 81/100] Iter[391/391]\t CE-loss: 1.1243\n","| Test Epoch #81\t Accuracy: 71.80%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 82/100] Iter[391/391]\t CE-loss: 1.1981\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 82/100] Iter[391/391]\t CE-loss: 1.0075\n","| Test Epoch #82\t Accuracy: 70.99%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 83/100] Iter[391/391]\t CE-loss: 1.0891\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 83/100] Iter[391/391]\t CE-loss: 1.1604\n","| Test Epoch #83\t Accuracy: 68.87%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 84/100] Iter[391/391]\t CE-loss: 1.0801\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 84/100] Iter[391/391]\t CE-loss: 1.1321\n","| Test Epoch #84\t Accuracy: 67.14%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 85/100] Iter[391/391]\t CE-loss: 1.1873\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 85/100] Iter[391/391]\t CE-loss: 1.4282\n","| Test Epoch #85\t Accuracy: 69.65%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 86/100] Iter[391/391]\t CE-loss: 1.1338\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 86/100] Iter[391/391]\t CE-loss: 1.1474\n","| Test Epoch #86\t Accuracy: 68.64%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 87/100] Iter[391/391]\t CE-loss: 1.2091\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 87/100] Iter[391/391]\t CE-loss: 1.0471\n","| Test Epoch #87\t Accuracy: 68.34%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 88/100] Iter[391/391]\t CE-loss: 1.2567\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 88/100] Iter[391/391]\t CE-loss: 1.2561\n","| Test Epoch #88\t Accuracy: 69.90%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 89/100] Iter[391/391]\t CE-loss: 1.3143\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 89/100] Iter[391/391]\t CE-loss: 1.1432\n","| Test Epoch #89\t Accuracy: 67.75%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 90/100] Iter[391/391]\t CE-loss: 1.2904\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 90/100] Iter[391/391]\t CE-loss: 1.1628\n","| Test Epoch #90\t Accuracy: 69.96%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 91/100] Iter[391/391]\t CE-loss: 1.2654\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 91/100] Iter[391/391]\t CE-loss: 1.1599\n","| Test Epoch #91\t Accuracy: 66.28%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 92/100] Iter[391/391]\t CE-loss: 0.6910\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 92/100] Iter[391/391]\t CE-loss: 1.0711\n","| Test Epoch #92\t Accuracy: 70.93%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 93/100] Iter[391/391]\t CE-loss: 1.1438\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 93/100] Iter[391/391]\t CE-loss: 0.9369\n","| Test Epoch #93\t Accuracy: 65.92%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 94/100] Iter[391/391]\t CE-loss: 1.0095\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 94/100] Iter[391/391]\t CE-loss: 1.2124\n","| Test Epoch #94\t Accuracy: 68.28%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 95/100] Iter[391/391]\t CE-loss: 0.8547\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 95/100] Iter[391/391]\t CE-loss: 1.2010\n","| Test Epoch #95\t Accuracy: 65.49%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 96/100] Iter[391/391]\t CE-loss: 0.9439\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 96/100] Iter[391/391]\t CE-loss: 1.0095\n","| Test Epoch #96\t Accuracy: 65.44%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 97/100] Iter[391/391]\t CE-loss: 0.9003\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 97/100] Iter[391/391]\t CE-loss: 1.0223\n","| Test Epoch #97\t Accuracy: 67.75%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 98/100] Iter[391/391]\t CE-loss: 0.8773\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 98/100] Iter[391/391]\t CE-loss: 1.1009\n","| Test Epoch #98\t Accuracy: 65.88%\n","\n","Warmup Net1\n","cifar10:0.5-sym | Epoch [ 99/100] Iter[391/391]\t CE-loss: 1.0890\n","Warmup Net2\n","cifar10:0.5-sym | Epoch [ 99/100] Iter[391/391]\t CE-loss: 1.0879\n","| Test Epoch #99\t Accuracy: 68.94%\n","\n","Train Net1\n","labeled data has a size of 39037\n","unlabeled data has a size of 10963\n","cifar10:0.5-sym | Epoch [100/100] Iter[610/610]\t Labeled loss: 1.7041  Unlabeled loss: 0.0147\n","Train Net2\n","labeled data has a size of 39582\n","unlabeled data has a size of 10418\n","cifar10:0.5-sym | Epoch [100/100] Iter[619/619]\t Labeled loss: 1.8438  Unlabeled loss: 0.0170\n","| Test Epoch #100\t Accuracy: 83.55%\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZfbWfrW5FKLz"},"source":[""],"id":"ZfbWfrW5FKLz","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qd_le1KO9I12","executionInfo":{"status":"ok","timestamp":1620890213135,"user_tz":-60,"elapsed":463,"user":{"displayName":"Xiliang He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6bak5pEgJC5_hHvho9fDsSh27vhZUOZYg8FCDOg=s64","userId":"00653517590094465678"}},"outputId":"5289f9ba-0f4c-4773-ef3d-a759e8b5c73b"},"source":["import os\n","\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/MixDivide')\n","\n","print('***获取当前目录***')\n","print(os.getcwd())"],"id":"Qd_le1KO9I12","execution_count":null,"outputs":[{"output_type":"stream","text":["***获取当前目录***\n","/content/drive/MyDrive/Colab Notebooks/MixDivide\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ICtqLg1F8CTS"},"source":[""],"id":"ICtqLg1F8CTS","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-6S19Xzo-Pg9"},"source":[""],"id":"-6S19Xzo-Pg9","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q1MV_8C3-e4x"},"source":[""],"id":"Q1MV_8C3-e4x","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h3E8UXEa-v3c","executionInfo":{"status":"ok","timestamp":1620890210211,"user_tz":-60,"elapsed":21425,"user":{"displayName":"Xiliang He","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj6bak5pEgJC5_hHvho9fDsSh27vhZUOZYg8FCDOg=s64","userId":"00653517590094465678"}},"outputId":"7189b7d9-9712-4883-db9f-f3a99551916c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"h3E8UXEa-v3c","execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JawjkuFc-wrt"},"source":[""],"id":"JawjkuFc-wrt","execution_count":null,"outputs":[]}]}